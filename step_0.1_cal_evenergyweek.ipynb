{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import time, datetime\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "import seaborn as sns\n",
    "import pickle, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import osmnx as ox\n",
    "import igraph as ig\n",
    "import itertools\n",
    "from compress_pickle import dump as cdump\n",
    "from compress_pickle import load as cload\n",
    "import networkx as nx\n",
    "from multiprocessing import Pool\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import geopandas as gpd\n",
    "import matplotlib.patches as mpatches\n",
    "import shapely.geometry as sgeom\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import contextily as cx\n",
    "import random as rand\n",
    "import scipy as sp\n",
    "import folium\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# 1. From Timegeo to Usertraj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userTrajectories():\n",
    "    userTraj_point = {}\n",
    "    userTraj_label = {}\n",
    "    userTraj_zipcode = {}\n",
    "    userTraj_timeSeg = {}\n",
    "    userTraj_time = {}\n",
    "\n",
    "    pointsInZipcode = pickle.load(open('data/census/pointsInZipcode.pkl', 'rb'))\n",
    "    dataPath_sim = 'data/timegeo_week/'\n",
    "    simFiles = [dataPath_sim + f for f in os.listdir(dataPath_sim) if os.path.isfile(os.path.join(dataPath_sim, f)) and 'txt' in f]\n",
    "    simFiles = sorted(simFiles)\n",
    "\n",
    "    # we only consider commuters here\n",
    "    userCount = 0; in_list = False\n",
    "    for f in simFiles:\n",
    "        data = open(f, 'r')\n",
    "        for line in data:\n",
    "            line = line.strip().split(' ')\n",
    "            if len(line) == 1:\n",
    "                userCount += 1\n",
    "                if userCount%10000 == 0:\n",
    "                    print(\"user : \", userCount)\n",
    "                perID = int(line[0].split('-')[0])\n",
    "                otherLocations = {}\n",
    "                if perID not in userTraj_point:\n",
    "                    userTraj_point[perID] = []\n",
    "                    userTraj_label[perID] = []\n",
    "                    userTraj_zipcode[perID] = []\n",
    "                    userTraj_timeSeg[perID] = []\n",
    "                    userTraj_time[perID] = []\n",
    "            else:\n",
    "                timestep = int(line[0])\n",
    "                stay_label = str(line[1])\n",
    "                lon = float(line[2])\n",
    "                lat = float(line[3])\n",
    "                if stay_label == 'o':\n",
    "                    if (lon, lat) not in otherLocations:\n",
    "                        stay_label = stay_label + str(len(otherLocations))  \n",
    "                        otherLocations[(lon, lat)] = stay_label\n",
    "                    else:\n",
    "                        stay_label = otherLocations[(lon, lat)]\n",
    "                try:\n",
    "                    zipcode = pointsInZipcode[(lon, lat)]\n",
    "                except:\n",
    "                    zipcode = ''\n",
    "                # save\n",
    "                userTraj_label[perID].append(stay_label)\n",
    "                userTraj_point[perID].append([lon, lat])\n",
    "                userTraj_zipcode[perID].append(zipcode)\n",
    "                userTraj_time[perID].append(timestep)\n",
    "\n",
    "    # save\n",
    "    pickle.dump(userTraj_point, open('results/userTraj_point_all.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(userTraj_label, open('results/userTraj_label_all.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(userTraj_zipcode, open('results/userTraj_zipcode_all.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(userTraj_time, open('results/userTraj_time_all.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "userTrajectories()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate Route "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Find the Nearest Node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_point = pickle.load(open('results/userTraj_point_all.pkl', 'rb'), encoding='bytes')\n",
    "traj_time = pickle.load(open('results/userTraj_time_all.pkl', 'rb'), encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_time = []; user_id = []\n",
    "user_keys = list(traj_point.keys())\n",
    "for key in user_keys:\n",
    "    od_time += traj_time[key][1:]\n",
    "    user_id += [key]*(len(traj_point[key])-1)\n",
    "od_time = np.array(od_time).reshape((-1,1))\n",
    "user_id = np.array(user_id).reshape((-1,1))\n",
    "od_lonlat = np.concatenate([np.hstack([traj_point[key][:-1],traj_point[key][1:]]).reshape((-1,4)) for key in user_keys])\n",
    "od_matrix = np.concatenate([user_id,od_time,od_lonlat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_lonlat = pd.DataFrame(np.concatenate([od_matrix[:,2:4], od_matrix[:,4:6]]))\n",
    "point_lonlat = point_lonlat.drop_duplicates(subset=[0,1]).rename(columns={0:'lon',1:'lat'})\n",
    "lon = point_lonlat['lon'].values; lat = point_lonlat['lat'].values\n",
    "nxg = nx.relabel.convert_node_labels_to_integers(pickle.load(open('data/timeNet/network_hour_'+str(7)+'.pkl', 'rb'), encoding='bytes'))\n",
    "near_point = ox.distance.nearest_nodes(nxg, lon, lat, return_dist=False)\n",
    "point_lonlat['node_id'] = near_point\n",
    "point_lonlat.to_csv('results/point_lonlat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_point = pd.DataFrame(od_matrix).rename(columns={0:'user_id',1:'od_time',2:'o_lon',3:'o_lat',4:'d_lon',5:'d_lat'})\n",
    "od_point = od_point.merge(point_lonlat, left_on=['o_lon','o_lat'], right_on=['lon','lat']).rename(columns={'node_id':'o_node_id'})\n",
    "od_point = od_point.merge(point_lonlat, left_on=['d_lon','d_lat'], right_on=['lon','lat']).rename(columns={'node_id':'d_node_id'})\n",
    "od_point = od_point[['user_id', 'od_time', 'o_lon', 'o_lat', 'd_lon', 'd_lat','o_node_id','d_node_id']]\n",
    "od_point.to_csv('results/od_point.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Find the Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_point = pd.read_csv('results/od_point.csv',index_col=0)\n",
    "od_route = od_point.drop_duplicates(subset=['o_node_id','d_node_id','od_time'])\n",
    "od_route['od_time'] = (od_route['od_time']/6).astype(int)\n",
    "od_route = od_route[['od_time','o_node_id','d_node_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hour in range(25):\n",
    "    nxg = pickle.load(open('data/timeNet/network_hour_'+str(hour)+'.pkl', 'rb'), encoding='bytes')\n",
    "    G_nx = nx.relabel.convert_node_labels_to_integers(nxg)\n",
    "    od_route_hour = od_route[od_route['od_time']==hour]\n",
    "    s = list(od_route_hour['o_node_id'].values)\n",
    "    t = list(od_route_hour['d_node_id'].values)\n",
    "    od_route_detail = ox.distance.shortest_path(G_nx, s, t, weight='avg_travel_time_sec', cpus=16)\n",
    "    cdump(od_route_detail, 'results/od_route_hour'+str(hour)+'.lzma', compression='lzma')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate Energy Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(args, func, n_cores):\n",
    "    pool = Pool(n_cores)\n",
    "    results = pool.map(func,args)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "\n",
    "def speed2consumption(speed,miles):\n",
    "    if speed < 1:\n",
    "        speed = 1\n",
    "    return speed_to_consumption[speed]*miles\n",
    "\n",
    "def distance2consumption(miles):\n",
    "    r = 0.342\n",
    "    return miles*r\n",
    "\n",
    "def graph2length(graph,s,t):\n",
    "    return graph[s][t][0]['length']*0.000621371\n",
    "\n",
    "def graph2time(graph,s,t):\n",
    "    return graph[s][t][0]['avg_travel_time_sec']/3600\n",
    "\n",
    "def calEnergy(hour):\n",
    "    try:\n",
    "        nxg = pickle.load(open('data/timeNet/network_hour_'+str(hour)+'.pkl', 'rb'), encoding='bytes'); GNSp = nx.relabel.convert_node_labels_to_integers(nxg)\n",
    "    except:\n",
    "        nxg = pickle.load(open('data/timeNet/network_hour_'+str(23)+'.pkl', 'rb'), encoding='bytes'); GNSp = nx.relabel.convert_node_labels_to_integers(nxg)\n",
    "    od_route_hour = cload('results/od_route_hour'+str(hour)+'.lzma', compression='lzma')\n",
    "    route_time = []; route_dis = []; route_energy = []\n",
    "\n",
    "    for route in od_route_hour:\n",
    "        time_total = 0; distance_total = 0; energy_total=0\n",
    "        for n in range(len(route)-1):\n",
    "            gdis = graph2length(GNSp,route[n],route[n+1])\n",
    "            gtime = graph2time(GNSp,route[n],route[n+1])\n",
    "            genergy = speed2consumption(int(gdis/gtime),gdis)\n",
    "            \n",
    "            time_total = time_total + gtime\n",
    "            distance_total = distance_total + gdis\n",
    "            energy_total = energy_total+ genergy\n",
    "\n",
    "        route_time.append(time_total)\n",
    "        route_dis.append(distance_total)\n",
    "        route_energy.append(energy_total)\n",
    "\n",
    "    pickle.dump(route_time, open('results/od_route_time_hour'+str(hour)+'.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(route_dis, open('results/od_route_dis_hour'+str(hour)+'.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(route_energy, open('results/od_route_energy_hour'+str(hour)+'.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "speed_to_consumption = pickle.load(open('results/speed_to_consumption.pkl', 'rb'), encoding='bytes')\n",
    "results = parallelize_dataframe([hour for hour in [24]],calEnergy, n_cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index\n",
    "od_point = pd.read_csv('results/od_point.csv',index_col=0)\n",
    "od_route = od_point.drop_duplicates(subset=['o_node_id','d_node_id','od_time'])\n",
    "od_route['od_time'] = (od_route['od_time']/6).astype(int)\n",
    "od_route = od_route[['od_time','o_node_id','d_node_id']]\n",
    "route_energy_oid = np.concatenate([od_route[od_route['od_time']==hour]['o_node_id'] for hour in range(25)])\n",
    "route_energy_did = np.concatenate([od_route[od_route['od_time']==hour]['d_node_id'] for hour in range(25)])\n",
    "route_energy_time = np.concatenate([od_route[od_route['od_time']==hour]['od_time'] for hour in range(25)])\n",
    "# Get the energy, dis, time\n",
    "route_energy_total = []\n",
    "route_time_total = []\n",
    "route_dis_total = []\n",
    "for hour in range(25):\n",
    "    route_energy = pickle.load(open('results/od_route_energy_hour'+str(hour)+'.pkl', 'rb'), encoding='bytes')\n",
    "    route_energy_total += route_energy\n",
    "    route_dis = pickle.load(open('results/od_route_dis_hour'+str(hour)+'.pkl', 'rb'), encoding='bytes')\n",
    "    route_dis_total += route_dis\n",
    "    route_time = pickle.load(open('results/od_route_time_hour'+str(hour)+'.pkl', 'rb'), encoding='bytes')\n",
    "    route_time_total += route_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_route_table = pd.DataFrame({'o_node_id':route_energy_oid,'d_node_id':route_energy_did,'od_time':route_energy_time,'energy':route_energy_total,'dis':route_dis_total,'time':route_time_total})\n",
    "od_route_table.to_csv('results/od_route_energy_table.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Map the OD with the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_point = pd.read_csv('results/od_point.csv',index_col=0)\n",
    "od_point['od_time_int'] = (od_point['od_time']/6).astype(int)\n",
    "od_route_table = pd.read_csv('results/od_route_energy_table.csv',index_col=0).drop_duplicates()\n",
    "od_route_results = od_point.merge(od_route_table,left_on=['od_time_int','o_node_id','d_node_id'],right_on=['od_time','o_node_id','d_node_id'])\n",
    "od_route_results.to_csv('results/od_route.csv')\n",
    "od_route_results_sort = od_route_results.sort_values(['user_id','od_time_x'])\n",
    "od_route_results_sort.to_csv('results/od_route_results_sort.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_route_results_sort = pd.read_csv('results/od_route_results_sort.csv',index_col=0)\n",
    "table = pd.pivot_table(od_route_results_sort, values='energy', index='user_id', aggfunc=np.sum)\n",
    "traj_energy = table.to_dict('index')\n",
    "pickle.dump(traj_energy, open('results/userTraj_energy_all.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "table = pd.pivot_table(od_route_results_sort, values='dis', index='user_id', aggfunc=np.sum)\n",
    "traj_dis = table.to_dict('index')\n",
    "pickle.dump(traj_dis, open('results/userTraj_rdis_all.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "table = pd.pivot_table(od_route_results_sort, values='time', index='user_id', aggfunc=np.sum)\n",
    "traj_time = table.to_dict('index')\n",
    "pickle.dump(traj_time, open('results/userTraj_rtime_all.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 19:58:26) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3242a12bd22337d578e6ee4799de02d8ca29de1afb6b4965caa7fc30b1aede52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
